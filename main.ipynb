{"cells":[{"cell_type":"markdown","metadata":{},"source":["### installing requiered libraries"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["!pip install deap"]},{"cell_type":"markdown","metadata":{},"source":["### imports"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import random\n","from deap import base, creator, tools\n","from tqdm import tqdm\n","from copy import deepcopy\n","import numpy as np\n","import pandas as pd\n","import torch\n","\n","import torch.nn as nn\n","from torch.utils.data import Dataset\n","from torch.utils.data import DataLoader\n","from PIL import Image\n","from imblearn.over_sampling import SMOTE\n","\n","import matplotlib.pyplot as plt\n","import torch.optim as optim\n","from torch.optim.lr_scheduler import ReduceLROnPlateau\n","import torch.nn.init as init\n","from torchvision.transforms import transforms,RandomRotation,ColorJitter,GaussianBlur,RandomHorizontalFlip,Resize,AutoAugment,AutoAugmentPolicy\n","import torchvision\n","from torch import Tensor\n","import torch.nn.functional as F\n","from PIL import Image\n","from sklearn.preprocessing import MinMaxScaler\n","from torch.utils.data.sampler import WeightedRandomSampler\n","import seaborn as sns\n","import copy\n","import cv2\n","from sklearn.preprocessing import normalize\n","from imblearn.over_sampling import SMOTE\n","import random\n","import os\n","\n","\n","np.random.seed(0)\n","# torch.backends.cudnn.deterministic = True\n","torch.manual_seed(0)\n","torch.cuda.manual_seed_all(0)\n","np.random.seed(0)"]},{"cell_type":"markdown","metadata":{},"source":["### fetching data from google drive in colab env"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/drive')\n","!unzip \"drive/MyDrive/data/dataset_name.zip\" -d ./data/\n","\n","ds_name='dataset_name'"]},{"cell_type":"markdown","metadata":{},"source":["### fetching data from google drive in kaggle env"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2025-02-16T08:22:15.710278Z","iopub.status.busy":"2025-02-16T08:22:15.709990Z","iopub.status.idle":"2025-02-16T08:23:08.318500Z","shell.execute_reply":"2025-02-16T08:23:08.317847Z","shell.execute_reply.started":"2025-02-16T08:22:15.710255Z"},"trusted":true},"outputs":[],"source":["from pydrive2.auth import GoogleAuth\n","from pydrive2.drive import GoogleDrive\n","from google.colab import auth\n","from oauth2client.client import GoogleCredentials\n","import shutil  \n","import os\n","import zipfile  \n","auth.authenticate_user()\n","gauth = GoogleAuth()\n","gauth.credentials = GoogleCredentials.get_application_default()\n","drive = GoogleDrive(gauth)\n","\n","ds_name='dataset_name'\n","downloaded = drive.CreateFile({'id': 'XXXXXXXXXXXXXXXXXX'})\n","os.makedirs('data', exist_ok=True)\n","downloaded.GetContentFile(f'data/{ds_name}.zip')\n","\n","with zipfile.ZipFile(f'data/{ds_name}.zip', 'r') as zip_ref:  \n","    zip_ref.extractall('data')  \n","os.remove(f'data/{ds_name}.zip')\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["### Init"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","batch_size = 32\n","n_classes=3\n","img_size=100\n","is_warming_up=False"]},{"cell_type":"markdown","metadata":{},"source":["### Prepare DataLoader"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2025-02-16T08:41:43.980439Z","iopub.status.busy":"2025-02-16T08:41:43.980147Z","iopub.status.idle":"2025-02-16T08:41:57.275147Z","shell.execute_reply":"2025-02-16T08:41:57.274397Z","shell.execute_reply.started":"2025-02-16T08:41:43.980416Z"},"trusted":true},"outputs":[],"source":["class CustomDataset(Dataset):\n","\n","    def __init__(self, imgs, lbls, transform=None):\n","        self.imgs = imgs\n","        self.lbls=torch.tensor(lbls, dtype=torch.long)\n","        self.transform = transform\n","\n","\n","    def __len__(self):\n","        return len(self.imgs)\n","\n","    def __getitem__(self, idx):\n","        img,lbl = self.imgs[idx], self.lbls[idx]\n","        img=Image.fromarray(img)\n","        if self.transform != None:\n","            img = self.transform(img)\n","        return img,lbl\n","\n","def string_to_array(x):\n","    result=np.array(x.split(' '),dtype=np.uint8).reshape(img_size,img_size)\n","    return result\n","def get_dataloaders(dataset_name,batch_size,img_size):\n","\n","\n","    df=pd.read_csv(f'./data/{dataset_name}.csv')\n","\n","    X = np.array(df['pixels'].apply(lambda x: string_to_array(x)).tolist())\n","\n","    y=df['lbl'].astype(np.uint8)\n","\n","\n","    train_mask = df['usage']=='train'\n","    valid_mask = df['usage']=='valid'\n","    test_mask = df['usage']=='test'\n","\n","\n","    x_train=X[train_mask]\n","    y_train=y[train_mask]\n","\n","    x_valid=X[valid_mask]\n","    y_valid=y[valid_mask]\n","\n","    x_test=X[test_mask]\n","    y_test=y[test_mask]\n","    \n","    # Balancing training data\n","    sm = SMOTE(random_state = 2)\n","    x_train, y_train = sm.fit_resample(x_train.reshape(x_train.shape[0], -1) , y_train)\n","    x_train=x_train.reshape(-1,48,48)\n","\n","    train_transform = transforms.Compose([\n","                transforms.RandomHorizontalFlip(p=0.5),\n","                Resize((img_size,img_size)),\n","                transforms.ToTensor(),\n","                transforms.Normalize(mean=[0.5], std=[0.5])\n","                ])\n","    valid_test_transform = transforms.Compose([\n","                Resize((img_size,img_size)),\n","                transforms.ToTensor(),  \n","                transforms.Normalize(mean=[0.5], std=[0.5])\n","                ])\n","\n","    dataset_train = CustomDataset(x_train,y_train,transform=train_transform)\n","    dataset_valid = CustomDataset(x_valid, y_valid,transform=valid_test_transform)\n","    dataset_test = CustomDataset(x_test, y_test,transform=valid_test_transform)\n","\n","    data_loaders_dict={}\n","    data_loaders_dict['train']=DataLoader(dataset_train, batch_size=batch_size,shuffle=True,num_workers=2)\n","    data_loaders_dict['valid']=DataLoader(dataset_valid, batch_size=batch_size,shuffle=True,num_workers=2)\n","\n","    data_loaders_dict['test'] = DataLoader(dataset_test, batch_size=batch_size,shuffle=True,num_workers=2)\n","\n","\n","    return data_loaders_dict\n","\n","\n","\n","dataloader_dict= get_dataloaders(ds_name,batch_size=batch_size,img_size=img_size)"]},{"cell_type":"markdown","metadata":{},"source":["### Model"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2025-02-16T09:54:31.229600Z","iopub.status.busy":"2025-02-16T09:54:31.229271Z","iopub.status.idle":"2025-02-16T09:54:31.278748Z","shell.execute_reply":"2025-02-16T09:54:31.277905Z","shell.execute_reply.started":"2025-02-16T09:54:31.229573Z"},"trusted":true},"outputs":[],"source":["class Simple_CNN(nn.Module):\n","\n","    def __init__(self,feature_extractor_layers,mlp_layers):\n","        super(Simple_CNN,self).__init__()\n","\n","        self.optimizer = None\n","        self.scheduler = None\n","        self.criterion = None\n","\n","        self.loss_acc_info={}\n","\n","        for validation_type in ['train','valid','test']:\n","\n","            self.loss_acc_info[validation_type]={}\n","\n","            for metric in ['acc','loss']:\n","                self.loss_acc_info[validation_type][metric]=[]\n","\n","        self.best_checkpoint={}\n","        for metric in ['acc','loss']:\n","            self.best_checkpoint[metric]={\n","            'best_acc_model_state_dicts':self.state_dict(),\n","            metric:0 if metric=='acc' else float('inf'),\n","\n","            }\n","\n","        self.feature_extractor=nn.Sequential(*feature_extractor_layers)\n","        self.flt = nn.Flatten()\n","        self.coarse_mlp_model=nn.Sequential(*mlp_layers)\n","\n","    def forward(self, x):\n","\n","        features=self.feature_extractor(x)\n","        flattened_features=self.flt(features)\n","        logits=self.coarse_mlp_model(flattened_features)\n","\n","        return logits\n","\n","    def init_weights(self,m):\n","        if isinstance(m, nn.Linear):\n","            init.xavier_uniform_(m.weight)\n","            if m.bias is not None:\n","                init.zeros_(m.bias)\n","\n","        if isinstance(m, nn.Conv2d):\n","            init.kaiming_uniform_(m.weight, nonlinearity='relu')\n","            if m.bias is not None:\n","                init.zeros_(m.bias)\n","\n","        \n","    def is_model_initialized(self):\n","        for name, param in self.named_parameters():\n","            if isinstance(param, nn.parameter.UninitializedParameter):\n","                print(f\"Uninitialized parameter found in: {name}\")\n","                return False\n","        print(\"All layers are initialized\")\n","        return True\n","\n","\n","    def calculate_loss(self,y_pred,y_target):\n","\n","        return self.criterion(y_pred, y_target)\n","\n","\n","    def eval_model(self,dl_dict):\n","        self.eval()\n","        with torch.no_grad():\n","\n","            for dl_type_temp , dl_temp in dl_dict.items():\n","                num_sample = []\n","                correct=[]\n","                running_loss=[]\n","                all_pred=[]\n","                all_target=[]\n","\n","\n","                for x,y in dl_temp:\n","\n","                    x = x.to(device)\n","                    y = y.to(device)\n","                    \n","                    logits= self(x)\n","\n","                    batch_loss=self.calculate_loss(y_pred=logits, y_target=y)\n","                    running_loss.append(batch_loss.item())\n","                    all_pred.extend(list(logits.detach().cpu().argmax(dim=1)))\n","                    all_target.extend(list(y.detach().cpu()))\n","\n","                    correct.append((y.detach().cpu()==logits.argmax(dim=1).detach().cpu()).sum().item())\n","                    num_sample.append(logits.shape[0])\n","\n","                self.loss_acc_info[dl_type_temp]['acc'].append(sum(correct)/sum(num_sample))\n","                self.loss_acc_info[dl_type_temp]['loss'].append(np.mean(running_loss))\n","\n","                avg_loss=self.loss_acc_info[dl_type_temp]['loss'][-1]\n","                \n","                if dl_type_temp == 'valid' :\n","                    self.scheduler.step(avg_loss)\n","\n","                acc=self.loss_acc_info[dl_type_temp]['acc'][-1]\n","                if dl_type_temp == 'test' and acc>=self.best_checkpoint['acc']['acc']:\n","\n","                    self.best_checkpoint['acc']['acc']=acc\n","                    self.best_checkpoint['model_state_dict_acc']=copy.deepcopy(self.state_dict())\n","\n","\n","                loss=self.loss_acc_info[dl_type_temp]['loss'][-1]\n","                if dl_type_temp == 'test' and loss<=self.best_checkpoint['loss']['loss']:\n","\n","                    self.best_checkpoint['loss']['loss']=loss\n","                    self.best_checkpoint['model_state_dict_acc']=copy.deepcopy(self.state_dict())\n","\n","    def one_step_train(self,dl):\n","        self.train()\n","\n","        for x,y in dl:\n","\n","            self.optimizer.zero_grad()\n","            \n","            x = x.cuda()\n","            y = y.cuda()\n","            \n","            logits= self(x)\n","\n","            batch_loss=self.calculate_loss(y_pred=logits, y_target=y)\n","\n","            batch_loss.backward()\n","            self.optimizer.step()\n","\n","    def train_model(self,num_epochs,dataloader_dict):\n","\n","        for epoch in tqdm(range(num_epochs)):\n","\n","            self.one_step_train(dataloader_dict['train'])\n","            self.eval_model(dataloader_dict)\n","\n","def create_layer(gen):\n","    config=gen.config\n","    layer_type=config['layer_type']\n","\n","    if layer_type=='conv':\n","        layer=nn.Sequential()\n","        layer.append(nn.LazyConv2d(config['num_feature_map'],kernel_size=config['kernel_size'],bias=False))\n","        layer.append(nn.LazyBatchNorm2d())\n","        layer.append(nn.ReLU())\n","\n","    elif layer_type=='deconv':\n","        layer=nn.Sequential()\n","        layer.append(nn.LazyConvTranspose2d(out_channels=config['num_feature_map'],kernel_size=config['kernel_size'],bias=False))\n","        layer.append(nn.LazyBatchNorm2d())\n","        layer.append(nn.ReLU())\n","\n","    elif layer_type=='identity':\n","        layer=nn.Identity()\n","\n","    elif layer_type=='maxpool':\n","        layer=nn.MaxPool2d((config['kernel_size'],config['stride']))\n","\n","    else:\n","        raise ValueError('invalid layer type')\n","\n","    return layer\n","def set_layers(ind):\n","        \n","    feature_extractor_layers=[]\n","    for idx in range(len(ind)-1):\n","        feature_extractor_layers.append(create_layer(ind[idx]))\n","    feature_extractor_layers.append(nn.AdaptiveAvgPool2d((1,1)))\n","\n","\n","    mlp_layers=[\n","        nn.Dropout1d(ind[-1].config['drop_rate']),\n","        nn.LazyLinear(n_classes)\n","    ]\n","    return feature_extractor_layers,mlp_layers\n","\n","\n","def get_acc_loss_nparam(ind,dataloader_dict,epoch):\n","    criterion = nn.CrossEntropyLoss()\n","\n","    try:\n","        my_model=Simple_CNN(*set_layers(ind)).to(device)\n","    \n","        my_model.criterion=criterion\n","        my_model.optimizer=optim.Adamw(my_model.parameters())\n","        my_model.scheduler=ReduceLROnPlateau(my_model.optimizer, mode='min', patience=3, factor=0.95)\n","        # warming up model\n","        temp_x = torch.randn(1,1,img_size,img_size).to(device)\n","        my_model(temp_x)    \n","        # my_model.is_model_initialized()\n","        \n","        my_model.apply(my_model.init_weights)\n","        num_params = sum(p.numel() for p in my_model.parameters())  \n","\n","        my_model.train_model(num_epochs=epoch,dataloader_dict=dataloader_dict)\n","        return (my_model.best_checkpoint['acc']['acc'],my_model.best_checkpoint['loss']['loss'],num_params)\n","\n","    except Exception as e:\n","        # print(\"An error occurred:\", e)\n","        return (-1.0,-1.0,-1.0)"]},{"cell_type":"markdown","metadata":{},"source":["### GA"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.execute_input":"2025-02-16T09:59:31.568861Z","iopub.status.busy":"2025-02-16T09:59:31.568504Z"},"id":"LL0Zdzq6QPw2","outputId":"f4592a5f-fe4e-4e6b-e84a-d1586c462b3e","trusted":true},"outputs":[],"source":["\n","config_seprator='*'\n","gen_seprator='***'\n","\n","class Gen():\n","    \n","    def __init__(self,config) -> None:\n","        self.config=config\n","\n","    def get_gen_name(self):\n","        gen_seq=''\n","\n","        for key,val in self.config.items():\n","            gen_seq=gen_seq+f'({key}_{val})'\n","\n","        gen_seq=gen_seq.replace( ')(',\n","                                 f'){config_seprator}('\n","                                 )\n","\n","        return gen_seq\n","    def __getitem__(self, index):\n","        \n","        return self.config[index] \n","    def __setitem__(self, index, value):  \n","        self.config[index] = value  \n","\n","class Chromosome():\n","\n","    def __init__(self,gens_list) -> None:\n","        self.gens_list=gens_list\n","\n","    def get_chromosome_name(self):\n","        chrom_seq=''\n","        for gen in self.gens_list:\n","            chrom_seq=chrom_seq+gen.get_gen_name()\n","        chrom_seq=chrom_seq.replace( ')(',\n","                          f'){gen_seprator}('\n","                        )\n","        \n","        return chrom_seq\n","\n","    def __iter__(self):  \n","        return iter(self.gens_list)\n","    def __getitem__(self, index):\n","        \n","        return self.gens_list[index] \n","\n","    def __len__(self):  \n","        return len(self.gens_list)\n","    def __setitem__(self, index, value):  \n","        self.gens_list[index] = value \n","\n","\n","creator.create(\"FitnessMax\", base.Fitness, weights=(1.0,))\n","creator.create(\"Individual\", Chromosome, fitness=creator.FitnessMax)\n","\n","\n","p_mutation = 0.2\n","p_cross = 0.8\n","n_generation = 20\n","n_run = 2\n","population_size = 10\n","tournament_size = 2\n","n_feature_extraction_layers=6\n","n_dropout_layers=1\n","epoch=2\n","config_names = [\n","    f'layer_{i}'\n","    for i in range(n_feature_extraction_layers)\n","]\n","config_names = config_names + [\n","    f'drop_{i}'\n","    for i in range(n_dropout_layers)\n","]\n","config_names+=['acc','loss','num_param']\n","feature_extraction_layer_types=['conv','deconv','identity','maxpool']\n","conv_kernel_size_range=[1,3,5]\n","conv_num_feature_map_range=[0,5]\n","dropout_range = [0,5]\n","maxpool_kernel_range=[2,3]\n","maxpool_stride_range=[2,3]\n","os.makedirs('output', exist_ok=True)\n","\n","config_path=f'output/{ds_name} configs.csv'\n","\n","\n","def create_individual():\n","\n","    while True:\n","        gens_list=[]\n","        for _ in range(n_feature_extraction_layers):\n","            layer_type=np.random.choice(feature_extraction_layer_types, size=1)[0]\n","            if layer_type in ['conv','deconv']:\n","                gen=Gen(\n","                    config=\n","                    {\n","                        'layer_type':layer_type,\n","                        'kernel_size':int(np.random.choice(conv_kernel_size_range, size=1)[0]),\n","                        'num_feature_map':pow(2, random.randrange(conv_num_feature_map_range[0], conv_num_feature_map_range[1]+1))\n","                    }\n","                )\n","            elif layer_type == 'maxpool':\n","                gen=Gen(\n","                    config=\n","                    {\n","                        'layer_type':layer_type,\n","                        'kernel_size':int(np.random.choice(maxpool_kernel_range, size=1)[0]),\n","                        'stride':int(np.random.choice(maxpool_stride_range, size=1)[0])\n","                    }\n","                )\n","            else:\n","                gen=Gen(\n","                    config=\n","                    {\n","                        'layer_type':layer_type\n","                    }\n","                )\n","\n","\n","            gens_list.append(gen)\n","\n","        for _ in range(n_dropout_layers):\n","            gen=Gen(\n","                    config=\n","                    {\n","                        'layer_type':'dropout',\n","                        'drop_rate':round(random.randrange(dropout_range[0], dropout_range[1]+1)*0.1, 1)\n","                    }\n","                )\n","            gens_list.append(gen)\n","        \n","        ind=Chromosome(gens_list=gens_list)\n","        if ind.get_chromosome_name() not in seen_fit:\n","            return ind  \n","\n","toolbox = base.Toolbox()\n","toolbox.register(\"individual\", tools.initIterate,\n","                 creator.Individual, create_individual)\n","toolbox.register(\"population\", tools.initRepeat, list, toolbox.individual)\n","\n","\n","def custom_mutation(x, mpb):\n","\n","    ind = deepcopy(x)\n","\n","    if random.random() < mpb:\n","        mutation_point = random.choice(range(len(ind)))\n","        \n","        if mutation_point<len(ind)-n_dropout_layers:\n","            layer_type=np.random.choice(feature_extraction_layer_types, size=1)[0]\n","            if layer_type in ['conv','deconv']:\n","                gen=Gen(\n","                    config=\n","                    {\n","                        'layer_type':layer_type,\n","                        'kernel_size':int(np.random.choice(conv_kernel_size_range, size=1)[0]),\n","                        'num_feature_map':pow(2, random.randrange(conv_num_feature_map_range[0], conv_num_feature_map_range[1]+1))\n","                    }\n","                )\n","            elif layer_type == 'maxpool':\n","                gen=Gen(\n","                    config=\n","                    {\n","                        'layer_type':layer_type,\n","                        'kernel_size':int(np.random.choice(maxpool_kernel_range, size=1)[0]),\n","                        'stride':int(np.random.choice(maxpool_stride_range, size=1)[0])\n","                    }\n","                )\n","            else:\n","                gen=Gen(\n","                    config=\n","                    {\n","                        'layer_type':layer_type\n","                    }\n","                )\n","            ind[mutation_point]=gen\n","\n","        else:\n","            gen=Gen(\n","                    config=\n","                    {\n","                        'layer_type':'dropout',\n","                        'drop_rate':round(random.randrange(dropout_range[0], dropout_range[1]+1)*0.1, 1)\n","                    }\n","                )\n","            ind[mutation_point] = gen\n","\n","    return ind\n","\n","\n","def custom_crossover(ind1, ind2, cxpb):\n","    if random.random() < cxpb:\n","\n","        c1, c2 = tools.cxOnePoint(ind1[:], ind2[:])\n","        c1, c2 = creator.Individual(c1), creator.Individual(c2)\n","        \n","        return c1, c2\n","    else:\n","        return creator.Individual(ind1[:]), creator.Individual(ind2[:])\n","\n","\n","def get_fitness(ind):\n","    \n","    ind_name = ind.get_chromosome_name()\n","    if ind_name in seen_fit:\n","        return (seen_fit[ind_name][0],)\n","    else:\n","        # fitnesses=(acc,loss,num_param)\n","        fitnesses = get_acc_loss_nparam(ind=ind,dataloader_dict=dataloader_dict,epoch=epoch)\n","        \n","        # print(fitnesses)\n","        seen_fit[ind_name] = fitnesses\n","        df_all = pd.DataFrame([ind_name.split(gen_seprator)+list(ind_fit) for ind_name,ind_fit in seen_fit.items()],\n","                              columns=config_names)\n","        df_all.to_csv(config_path, index=False)\n","\n","        return (fitnesses[0],)\n","\n","def eval_and_check_duplicate(inds_list,traget_list=None):\n","    if traget_list is not None:\n","        target_name_list=[t.get_chromosome_name() for t in traget_list ]\n","    else:\n","        target_name_list=[]\n","    for idx in range(len(inds_list)):\n","        is_ok=False\n","        while not is_ok:\n","            fitness=toolbox.evaluate(inds_list[idx])\n","            if (inds_list[idx].get_chromosome_name() in target_name_list) or fitness[0]==-1:\n","                inds_list[idx]=toolbox.population(n=1)[0] \n","            else:\n","                inds_list[idx].fitness.values = fitness\n","                is_ok=True\n","    return inds_list\n","\n","\n","toolbox.register(\"select\", tools.selTournament, tournsize=tournament_size)\n","\n","toolbox.register(\"evaluate\", get_fitness)\n","\n","toolbox.register(\"mate\", custom_crossover, cxpb=p_cross)\n","\n","toolbox.register(\"mutate\", custom_mutation, mpb=p_mutation)\n","\n","max_acc_per_run = []\n","\n","seen_fit = {}\n","\n","# load previous inds\n","print('Previous ckp: ',os.path.isfile(config_path))\n","if os.path.isfile(config_path):\n","    df = pd.read_csv(config_path)\n","    for temp_data in df.iterrows():\n","        ind_name=gen_seprator.join(temp_data[1].values[:-3])\n","\n","        seen_fit[ind_name] = ((float(temp_data[1].values[-3]),float(temp_data[1].values[-2]),int(temp_data[1].values[-1])))\n","\n","    print(f'Previous ckp is loaded {len(seen_fit)=}')\n","\n","\n","for _ in range(n_run):\n","\n","    population = toolbox.population(n=population_size)\n","\n","    for idx in range(len(population)):\n","        population=eval_and_check_duplicate(population,None)\n","    \n","    for current_gen in range(n_generation):\n","        print(f'{current_gen=}')\n","        \n","        fitnesses = [ind.fitness.values[0] for ind in population]\n","\n","        print(f'best ind:{population[np.argmax(fitnesses)].get_chromosome_name()} \\nfittness :{np.max(fitnesses)}')\n","\n","        offspring = []\n","\n","        while len(offspring) < population_size:\n","\n","            ind1, ind2 = random.sample(population, 2)\n","            child1, child2 = toolbox.mate(ind1, ind2)\n","\n","            child1 = toolbox.mutate(child1)\n","            child2 = toolbox.mutate(child2)\n","            \n","            new_children=[child1,child2]\n","            new_children=eval_and_check_duplicate(new_children,population + offspring)\n","\n","            offspring.extend(new_children)\n","\n","        candidate_for_next_generation=population + offspring\n","        \n","        elitism_ind =candidate_for_next_generation.pop(np.argmax(\n","            [ind.fitness.values[0] for ind in (population + offspring)]))\n","        population = toolbox.select(\n","            candidate_for_next_generation, int(0.8*population_size))\n","\n","        population.append(elitism_ind)\n","        new_rand_inds=toolbox.population(n=population_size-len(population))\n","        new_rand_inds=eval_and_check_duplicate(new_rand_inds,population)\n","\n","        population.extend(new_rand_inds)\n","    max_acc_per_run.append(max([ind.fitness.values[0] for ind in population]))\n","\n","fitnesses = [ind.fitness.values[0] for ind in population]\n","\n","print(f'best ind:{population[np.argmax(fitnesses)].get_chromosome_name()} \\nfittness :{np.max(fitnesses)}')\n","\n"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30887,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.7"}},"nbformat":4,"nbformat_minor":4}
